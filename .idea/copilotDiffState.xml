<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/rag_bdd.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/rag_bdd.py" />
              <option name="originalContent" value="#!/usr/bin/env python3&#10;&quot;&quot;&quot;&#10;rag_bdd.py&#10;Simple RAG tool for BDD feature files:&#10;- parse .feature files into scenario chunks&#10;- index into Chroma (HuggingFace embeddings via LangChain)&#10;- rewrite feature(s) in descriptive style preserving exact phrases&#10;- create new scenario from requirement and list matching scenarios&#10;&quot;&quot;&quot;&#10;&#10;import os&#10;import re&#10;import argparse&#10;from typing import List, Dict, Tuple&#10;from langchain.schema import Document&#10;from langchain.embeddings import HuggingFaceEmbeddings&#10;from langchain.vectorstores import Chroma&#10;from langchain.chat_models import ChatOpenAI&#10;&#10;# rag_bdd.py&#10;&#10;from dotenv import load_dotenv&#10;import os&#10;from langchain.chat_models import ChatOpenAI&#10;&#10;# 1. Load environment variables from .env file&#10;load_dotenv()&#10;&#10;# 2. Fetch API key&#10;api_key = os.getenv(&quot;OPENAI_API_KEY&quot;)&#10;&#10;# 3. Initialize OpenAI client&#10;llm = ChatOpenAI(&#10;    model_name=&quot;gpt-3.5-turbo&quot;,&#10;    temperature=0,&#10;    openai_api_key=&quot;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&quot;&#10;)&#10;&#10;# rest of your imports and RAG pipeline code go here...&#10;&#10;&#10;# -----------------------&#10;# Parser: Feature -&gt; Scenarios (simple, regex-based)&#10;# -----------------------&#10;def parse_feature_file(path: str) -&gt; List[Dict]:&#10;    &quot;&quot;&quot;&#10;    Parse a Gherkin .feature file into a list of scenario dicts:&#10;    { 'feature': ..., 'scenario': ..., 'steps': [...], 'raw': ..., 'file': path }&#10;    This is intentionally simple and assumes standard Gherkin keywords in English.&#10;    &quot;&quot;&quot;&#10;    scenarios = []&#10;    current_feature = None&#10;    current_scenario = None&#10;    current_steps = []&#10;    raw_lines = []&#10;    with open(path, 'r', encoding='utf-8') as f:&#10;        lines = f.readlines()&#10;&#10;    for line in lines:&#10;        raw_lines.append(line)&#10;        m_feat = re.match(r'^\s*Feature:\s*(.+)', line, re.IGNORECASE)&#10;        m_scn = re.match(r'^\s*Scenario(?: Outline)?:\s*(.+)', line, re.IGNORECASE)&#10;        m_step = re.match(r'^\s*(Given|When|Then|And|But)\b(.*)', line, re.IGNORECASE)&#10;        if m_feat:&#10;            current_feature = m_feat.group(1).strip()&#10;        elif m_scn:&#10;            # push previous scenario&#10;            if current_scenario:&#10;                scenarios.append({&#10;                    'feature': current_feature,&#10;                    'scenario': current_scenario,&#10;                    'steps': [s.strip() for s in current_steps],&#10;                    'raw': '\n'.join(raw_lines),  # raw file; useful for reference&#10;                    'file': path&#10;                })&#10;                current_steps = []&#10;            current_scenario = m_scn.group(1).strip()&#10;        elif m_step and current_scenario:&#10;            # preserve the whole step line (keyword + rest)&#10;            step_line = line.strip()&#10;            current_steps.append(step_line)&#10;        # else ignore other lines for now (Examples, background, comments handled loosely)&#10;&#10;    # push last scenario&#10;    if current_scenario:&#10;        scenarios.append({&#10;            'feature': current_feature,&#10;            'scenario': current_scenario,&#10;            'steps': [s.strip() for s in current_steps],&#10;            'raw': '\n'.join(raw_lines),&#10;            'file': path&#10;        })&#10;    return scenarios&#10;&#10;def parse_path(path: str) -&gt; List[Dict]:&#10;    &quot;&quot;&quot;&#10;    Accept either a single .feature file or a directory of .feature files.&#10;    Returns combined list of scenario dicts.&#10;    &quot;&quot;&quot;&#10;    scen_list = []&#10;    if os.path.isdir(path):&#10;        for root, _, files in os.walk(path):&#10;            for fn in files:&#10;                if fn.endswith('.feature'):&#10;                    scen_list.extend(parse_feature_file(os.path.join(root, fn)))&#10;    elif os.path.isfile(path) and path.endswith('.feature'):&#10;        scen_list = parse_feature_file(path)&#10;    else:&#10;        raise ValueError(&quot;Please provide a .feature file or directory containing .feature files.&quot;)&#10;    return scen_list&#10;&#10;# -----------------------&#10;# Build index&#10;# -----------------------&#10;def build_index(scenarios: List[Dict], persist_dir: str = &quot;./chroma_db&quot;):&#10;    &quot;&quot;&quot;&#10;    Convert scenario list to LangChain Documents and index with Chroma.&#10;    &quot;&quot;&quot;&#10;    docs = []&#10;    for i, s in enumerate(scenarios):&#10;        content = f&quot;Feature: {s.get('feature')}\nScenario: {s.get('scenario')}\n&quot; + &quot;\n&quot;.join(s.get('steps', []))&#10;        metadata = {&#10;            'file': s.get('file'),&#10;            'feature': s.get('feature'),&#10;            'scenario': s.get('scenario'),&#10;            'id': f&quot;{os.path.basename(s['file'])}::scenario::{i}&quot;&#10;        }&#10;        docs.append(Document(page_content=content, metadata=metadata))&#10;&#10;    emb = HuggingFaceEmbeddings(model_name=&quot;all-MiniLM-L6-v2&quot;)  # local embeddings (requires sentence-transformers)&#10;    vectordb = Chroma.from_documents(docs, emb, persist_directory=persist_dir)&#10;    vectordb.persist()&#10;    print(f&quot;Indexed {len(docs)} scenario documents into Chroma at {persist_dir}&quot;)&#10;    return vectordb&#10;&#10;# -----------------------&#10;# Utilities: retrieval + step extraction&#10;# -----------------------&#10;def extract_steps_from_text(text: str) -&gt; List[str]:&#10;    return re.findall(r'^\s*(Given|When|Then|And|But)[^\n]*', text, flags=re.IGNORECASE | re.MULTILINE)&#10;&#10;def find_matches(vectordb: Chroma, query: str, k: int = 5) -&gt; List[Dict]:&#10;    &quot;&quot;&quot;&#10;    Returns a list of matches with score, metadata, and a summary (first step or truncated scenario).&#10;    &quot;&quot;&quot;&#10;    results = vectordb.similarity_search_with_score(query, k=k)&#10;    matches = []&#10;    for doc, score in results:&#10;        steps = extract_steps_from_text(doc.page_content)&#10;        summary = steps[0] if steps else doc.page_content[:80] + (&quot;...&quot; if len(doc.page_content) &gt; 80 else &quot;&quot;)&#10;        matches.append({&#10;            'score': score,&#10;            'text': doc.page_content,&#10;            'metadata': doc.metadata,&#10;            'steps': steps,&#10;            'summary': summary&#10;        })&#10;    return matches&#10;&#10;# -----------------------&#10;# LLM prompts &amp; calls&#10;# -----------------------&#10;def call_llm(prompt: str, model_name=&quot;gpt-3.5-turbo&quot;, temperature=0.0) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    Uses LangChain ChatOpenAI wrapper. Make sure OPENAI_API_KEY is set in env.&#10;    Uses .invoke() for robust compatibility.&#10;    &quot;&quot;&quot;&#10;    llm = ChatOpenAI(model_name=model_name, temperature=temperature)&#10;    result = llm.invoke(prompt)&#10;    # Robust extraction for different LangChain versions&#10;    if isinstance(result, dict) and 'content' in result:&#10;        return result['content']&#10;    elif hasattr(result, 'content'):&#10;        return result.content&#10;    elif isinstance(result, str):&#10;        return result&#10;    else:&#10;        return str(result)&#10;&#10;# Prompt templates&#10;REWRITE_PROMPT = &quot;&quot;&quot;You are an expert BDD engineer. Re-write the ORIGIN_SCENARIO below into a descriptive style BDD scenario (Gherkin format).&#10;Rules:&#10;- Keep these exact phrases (case + whitespace) unchanged wherever applicable: &#10;{preserve_list}&#10;- Use the CONTEXT snippets below to remain consistent with existing naming and phrasing.&#10;- Keep the Gherkin keywords (Feature, Scenario, Given/When/Then/And/But).&#10;- Produce only the rewritten scenario (Feature &amp; Scenario &amp; Steps), no extra commentary.&#10;&#10;CONTEXT:&#10;{context}&#10;&#10;ORIGIN_SCENARIO:&#10;{origin}&#10;&quot;&quot;&quot;&#10;&#10;NEW_SCENARIO_PROMPT = &quot;&quot;&quot;You are an expert BDD engineer. Create a new BDD Scenario (Gherkin) that satisfies the requirement below.&#10;Rules:&#10;- Keep these exact phrases unchanged: {preserve_list}&#10;- Use the CONTEXT snippets below to maintain consistent naming/phrasing.&#10;- Provide a short header &quot;Matches:&quot; that lists matching existing scenarios (filename::scenario title) with similarity scores.&#10;- Output only the Gherkin Scenario (Feature + Scenario + Steps) followed by the Matches listing.&#10;&#10;CONTEXT:&#10;{context}&#10;&#10;REQUIREMENT:&#10;{requirement}&#10;&quot;&quot;&quot;&#10;&#10;# -----------------------&#10;# High-level operations&#10;# -----------------------&#10;def rewrite_feature_file(path: str, vectordb: Chroma, output_path: str = None):&#10;    &quot;&quot;&quot;&#10;    Read one feature file, parse scenarios, for each scenario retrieve context and ask LLM to rewrite descriptively.&#10;    &quot;&quot;&quot;&#10;    scenarios = parse_feature_file(path)&#10;    rewritten_sections = []&#10;    for s in scenarios:&#10;        origin = f&quot;Feature: {s['feature']}\nScenario: {s['scenario']}\n&quot; + &quot;\n&quot;.join(s['steps'])&#10;        # Find matches for the scenario text&#10;        matches = find_matches(vectordb, origin, k=5)&#10;        # collect preserve phrases (unique step lines from matches)&#10;        preserve = set()&#10;        context_snippets = []&#10;        for m in matches:&#10;            context_snippets.append(f&quot;[{m['metadata']['file']}::{m['metadata']['scenario']}] {m['text']}&quot;)&#10;            for st in m['steps']:&#10;                # keep only meaningful short steps; strip leading whitespace&#10;                preserve.add(st.strip())&#10;        preserve_list = &quot;\n&quot;.join(sorted(preserve))&#10;        context = &quot;\n\n&quot;.join(context_snippets) if context_snippets else &quot;&quot;&#10;        prompt = REWRITE_PROMPT.format(preserve_list=preserve_list, context=context, origin=origin)&#10;        rewritten = call_llm(prompt)&#10;        rewritten_sections.append(rewritten.strip())&#10;        print(f&quot;Rewrote scenario: {s['scenario']}&quot;)&#10;    out = &quot;\n\n&quot;.join(rewritten_sections)&#10;    if not output_path:&#10;        output_path = path.replace(&quot;.feature&quot;, &quot;.descriptive.feature&quot;)&#10;    # Ensure output directory exists&#10;    out_dir = os.path.dirname(output_path)&#10;    if out_dir and not os.path.exists(out_dir):&#10;        os.makedirs(out_dir, exist_ok=True)&#10;    with open(output_path, 'w', encoding='utf-8') as f:&#10;        f.write(out)&#10;    print(f&quot;Wrote rewritten file to {output_path}&quot;)&#10;&#10;&#10;def new_scenario_from_requirement(requirement: str, vectordb: Chroma, top_k: int = 5, output_path: str = None):&#10;    &quot;&quot;&quot;&#10;    Given a textual requirement, retrieve matches and ask LLM to generate a new, consistent scenario.&#10;    Also returns the list of matching existing scenarios, with a consistency check.&#10;    &quot;&quot;&quot;&#10;    matches = find_matches(vectordb, requirement, k=top_k)&#10;    preserve = set()&#10;    context_snippets = []&#10;    matches_list = []&#10;    print(&quot;\n--- Consistency Check: Similar Existing Scenarios ---&quot;)&#10;    threshold = 0.8  # similarity threshold for warning&#10;    found_high = False&#10;    for m in matches:&#10;        context_snippets.append(f&quot;[{m['metadata']['file']}::{m['metadata']['scenario']}] {m['text']}&quot;)&#10;        matches_list.append(f&quot;{m['metadata']['file']}::{m['metadata']['scenario']} (score={m['score']:.4f})&quot;)&#10;        for st in m['steps']:&#10;            preserve.add(st.strip())&#10;        print(f&quot;- Feature: {m['metadata'].get('feature', '')}&quot;)&#10;        print(f&quot;  Scenario: {m['metadata'].get('scenario', '')}&quot;)&#10;        print(f&quot;  Score: {m['score']:.4f}&quot;)&#10;        print(f&quot;  Summary: {m['summary']}&quot;)&#10;        print()&#10;        if m['score'] &gt;= threshold:&#10;            found_high = True&#10;    if found_high:&#10;        print(&quot;WARNING: One or more similar scenarios have a high similarity score. This requirement may already be covered.\n&quot;)&#10;    prompt = NEW_SCENARIO_PROMPT.format(&#10;        preserve_list=&quot;\n&quot;.join(sorted(preserve)),&#10;        context=&quot;\n\n&quot;.join(context_snippets),&#10;        requirement=requirement&#10;    )&#10;    generated = call_llm(prompt)&#10;    # Save or print&#10;    if output_path:&#10;        with open(output_path, 'w', encoding='utf-8') as f:&#10;            f.write(generated)&#10;        print(f&quot;Wrote generated scenario to {output_path}&quot;)&#10;    print(&quot;\n--- Generated Scenario ---\n&quot;)&#10;    print(generated)&#10;    print(&quot;\n--- Matching existing scenarios (top results) ---&quot;)&#10;    for m in matches_list:&#10;        print(&quot;-&quot;, m)&#10;&#10;&#10;# -----------------------&#10;# CLI&#10;# -----------------------&#10;def main():&#10;    parser = argparse.ArgumentParser(description=&quot;RAG BDD tool&quot;)&#10;    sub = parser.add_subparsers(dest='cmd')&#10;&#10;    p_idx = sub.add_parser('index', help='Index .feature files into Chroma')&#10;    p_idx.add_argument('--path', required=True, help='.feature file or directory')&#10;    p_idx.add_argument('--persist', default='./chroma_db', help='Chroma persist directory')&#10;&#10;    p_rewrite = sub.add_parser('rewrite', help='Rewrite a feature file descriptively')&#10;    p_rewrite.add_argument('--file', required=True, help='path to feature file to rewrite')&#10;    p_rewrite.add_argument('--persist', default='./chroma_db', help='Chroma persist directory')&#10;    p_rewrite.add_argument('--out', default=None, help='output file')&#10;&#10;    p_new = sub.add_parser('new', help='Create new scenario from requirement')&#10;    p_new.add_argument('--requirement', required=True, help='requirement text')&#10;    p_new.add_argument('--persist', default='./chroma_db', help='Chroma persist directory')&#10;    p_new.add_argument('--top', type=int, default=5, help='top-k matches')&#10;    p_new.add_argument('--out', default=None, help='output file to save scenario')&#10;&#10;    args = parser.parse_args()&#10;    if args.cmd == 'index':&#10;        scenarios = parse_path(args.path)&#10;        build_index(scenarios, persist_dir=args.persist)&#10;    elif args.cmd == 'rewrite':&#10;        # load vectordb&#10;        emb = HuggingFaceEmbeddings(model_name=&quot;all-MiniLM-L6-v2&quot;)&#10;        vectordb = Chroma(persist_directory=args.persist, embedding_function=emb)&#10;        rewrite_feature_file(args.file, vectordb, output_path=args.out)&#10;    elif args.cmd == 'new':&#10;        emb = HuggingFaceEmbeddings(model_name=&quot;all-MiniLM-L6-v2&quot;)&#10;        vectordb = Chroma(persist_directory=args.persist, embedding_function=emb)&#10;        new_scenario_from_requirement(args.requirement, vectordb, top_k=args.top, output_path=args.out)&#10;    else:&#10;        parser.print_help()&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()&#10;" />
              <option name="updatedContent" value="#!/usr/bin/env python3&#10;&quot;&quot;&quot;&#10;rag_bdd.py&#10;Simple RAG tool for BDD feature files:&#10;- parse .feature files into scenario chunks&#10;- index into Chroma (HuggingFace embeddings via LangChain)&#10;- rewrite feature(s) in descriptive style preserving exact phrases&#10;- create new scenario from requirement and list matching scenarios&#10;&quot;&quot;&quot;&#10;&#10;import os&#10;import re&#10;import argparse&#10;from typing import List, Dict, Tuple&#10;from langchain.schema import Document&#10;from langchain.embeddings import HuggingFaceEmbeddings&#10;from langchain.vectorstores import Chroma&#10;from langchain.chat_models import ChatOpenAI&#10;&#10;# rag_bdd.py&#10;&#10;from dotenv import load_dotenv&#10;import os&#10;from langchain.chat_models import ChatOpenAI&#10;&#10;# 1. Load environment variables from .env file&#10;load_dotenv()&#10;&#10;# 2. Fetch API key&#10;api_key = os.getenv(&quot;OPENAI_API_KEY&quot;)&#10;&#10;# 3. Initialize OpenAI client&#10;llm = ChatOpenAI(&#10;    model_name=&quot;gpt-3.5-turbo&quot;,&#10;    temperature=0,&#10;    openai_api_key=&quot;&lt;enter&gt;&quot;&#10;)&#10;&#10;# rest of your imports and RAG pipeline code go here...&#10;&#10;&#10;# -----------------------&#10;# Parser: Feature -&gt; Scenarios (simple, regex-based)&#10;# -----------------------&#10;def parse_feature_file(path: str) -&gt; List[Dict]:&#10;    &quot;&quot;&quot;&#10;    Parse a Gherkin .feature file into a list of scenario dicts:&#10;    { 'feature': ..., 'scenario': ..., 'steps': [...], 'raw': ..., 'file': path }&#10;    This is intentionally simple and assumes standard Gherkin keywords in English.&#10;    &quot;&quot;&quot;&#10;    scenarios = []&#10;    current_feature = None&#10;    current_scenario = None&#10;    current_steps = []&#10;    raw_lines = []&#10;    with open(path, 'r', encoding='utf-8') as f:&#10;        lines = f.readlines()&#10;&#10;    for line in lines:&#10;        raw_lines.append(line)&#10;        m_feat = re.match(r'^\s*Feature:\s*(.+)', line, re.IGNORECASE)&#10;        m_scn = re.match(r'^\s*Scenario(?: Outline)?:\s*(.+)', line, re.IGNORECASE)&#10;        m_step = re.match(r'^\s*(Given|When|Then|And|But)\b(.*)', line, re.IGNORECASE)&#10;        if m_feat:&#10;            current_feature = m_feat.group(1).strip()&#10;        elif m_scn:&#10;            # push previous scenario&#10;            if current_scenario:&#10;                scenarios.append({&#10;                    'feature': current_feature,&#10;                    'scenario': current_scenario,&#10;                    'steps': [s.strip() for s in current_steps],&#10;                    'raw': '\n'.join(raw_lines),  # raw file; useful for reference&#10;                    'file': path&#10;                })&#10;                current_steps = []&#10;            current_scenario = m_scn.group(1).strip()&#10;        elif m_step and current_scenario:&#10;            # preserve the whole step line (keyword + rest)&#10;            step_line = line.strip()&#10;            current_steps.append(step_line)&#10;        # else ignore other lines for now (Examples, background, comments handled loosely)&#10;&#10;    # push last scenario&#10;    if current_scenario:&#10;        scenarios.append({&#10;            'feature': current_feature,&#10;            'scenario': current_scenario,&#10;            'steps': [s.strip() for s in current_steps],&#10;            'raw': '\n'.join(raw_lines),&#10;            'file': path&#10;        })&#10;    return scenarios&#10;&#10;def parse_path(path: str) -&gt; List[Dict]:&#10;    &quot;&quot;&quot;&#10;    Accept either a single .feature file or a directory of .feature files.&#10;    Returns combined list of scenario dicts.&#10;    &quot;&quot;&quot;&#10;    scen_list = []&#10;    if os.path.isdir(path):&#10;        for root, _, files in os.walk(path):&#10;            for fn in files:&#10;                if fn.endswith('.feature'):&#10;                    scen_list.extend(parse_feature_file(os.path.join(root, fn)))&#10;    elif os.path.isfile(path) and path.endswith('.feature'):&#10;        scen_list = parse_feature_file(path)&#10;    else:&#10;        raise ValueError(&quot;Please provide a .feature file or directory containing .feature files.&quot;)&#10;    return scen_list&#10;&#10;# -----------------------&#10;# Build index&#10;# -----------------------&#10;def build_index(scenarios: List[Dict], persist_dir: str = &quot;./chroma_db&quot;):&#10;    &quot;&quot;&quot;&#10;    Convert scenario list to LangChain Documents and index with Chroma.&#10;    &quot;&quot;&quot;&#10;    docs = []&#10;    for i, s in enumerate(scenarios):&#10;        content = f&quot;Feature: {s.get('feature')}\nScenario: {s.get('scenario')}\n&quot; + &quot;\n&quot;.join(s.get('steps', []))&#10;        metadata = {&#10;            'file': s.get('file'),&#10;            'feature': s.get('feature'),&#10;            'scenario': s.get('scenario'),&#10;            'id': f&quot;{os.path.basename(s['file'])}::scenario::{i}&quot;&#10;        }&#10;        docs.append(Document(page_content=content, metadata=metadata))&#10;&#10;    emb = HuggingFaceEmbeddings(model_name=&quot;all-MiniLM-L6-v2&quot;)  # local embeddings (requires sentence-transformers)&#10;    vectordb = Chroma.from_documents(docs, emb, persist_directory=persist_dir)&#10;    vectordb.persist()&#10;    print(f&quot;Indexed {len(docs)} scenario documents into Chroma at {persist_dir}&quot;)&#10;    return vectordb&#10;&#10;# -----------------------&#10;# Utilities: retrieval + step extraction&#10;# -----------------------&#10;def extract_steps_from_text(text: str) -&gt; List[str]:&#10;    return re.findall(r'^\s*(Given|When|Then|And|But)[^\n]*', text, flags=re.IGNORECASE | re.MULTILINE)&#10;&#10;def find_matches(vectordb: Chroma, query: str, k: int = 5) -&gt; List[Dict]:&#10;    &quot;&quot;&quot;&#10;    Returns a list of matches with score, metadata, and a summary (first step or truncated scenario).&#10;    &quot;&quot;&quot;&#10;    results = vectordb.similarity_search_with_score(query, k=k)&#10;    matches = []&#10;    for doc, score in results:&#10;        steps = extract_steps_from_text(doc.page_content)&#10;        summary = steps[0] if steps else doc.page_content[:80] + (&quot;...&quot; if len(doc.page_content) &gt; 80 else &quot;&quot;)&#10;        matches.append({&#10;            'score': score,&#10;            'text': doc.page_content,&#10;            'metadata': doc.metadata,&#10;            'steps': steps,&#10;            'summary': summary&#10;        })&#10;    return matches&#10;&#10;# -----------------------&#10;# LLM prompts &amp; calls&#10;# -----------------------&#10;def call_llm(prompt: str, model_name=&quot;gpt-3.5-turbo&quot;, temperature=0.0) -&gt; str:&#10;    &quot;&quot;&quot;&#10;    Uses LangChain ChatOpenAI wrapper. Make sure OPENAI_API_KEY is set in env.&#10;    Uses .invoke() for robust compatibility.&#10;    &quot;&quot;&quot;&#10;    llm = ChatOpenAI(model_name=model_name, temperature=temperature)&#10;    result = llm.invoke(prompt)&#10;    # Robust extraction for different LangChain versions&#10;    if isinstance(result, dict) and 'content' in result:&#10;        return result['content']&#10;    elif hasattr(result, 'content'):&#10;        return result.content&#10;    elif isinstance(result, str):&#10;        return result&#10;    else:&#10;        return str(result)&#10;&#10;# Prompt templates&#10;REWRITE_PROMPT = &quot;&quot;&quot;You are an expert BDD engineer. Re-write the ORIGIN_SCENARIO below into a descriptive style BDD scenario (Gherkin format).&#10;Rules:&#10;- Keep these exact phrases (case + whitespace) unchanged wherever applicable: &#10;{preserve_list}&#10;- Use the CONTEXT snippets below to remain consistent with existing naming and phrasing.&#10;- Keep the Gherkin keywords (Feature, Scenario, Given/When/Then/And/But).&#10;- Produce only the rewritten scenario (Feature &amp; Scenario &amp; Steps), no extra commentary.&#10;&#10;CONTEXT:&#10;{context}&#10;&#10;ORIGIN_SCENARIO:&#10;{origin}&#10;&quot;&quot;&quot;&#10;&#10;NEW_SCENARIO_PROMPT = &quot;&quot;&quot;You are an expert BDD engineer. Create a new BDD Scenario (Gherkin) that satisfies the requirement below.&#10;Rules:&#10;- Keep these exact phrases unchanged: {preserve_list}&#10;- Use the CONTEXT snippets below to maintain consistent naming/phrasing.&#10;- Provide a short header &quot;Matches:&quot; that lists matching existing scenarios (filename::scenario title) with similarity scores.&#10;- Output only the Gherkin Scenario (Feature + Scenario + Steps) followed by the Matches listing.&#10;&#10;CONTEXT:&#10;{context}&#10;&#10;REQUIREMENT:&#10;{requirement}&#10;&quot;&quot;&quot;&#10;&#10;# -----------------------&#10;# High-level operations&#10;# -----------------------&#10;def rewrite_feature_file(path: str, vectordb: Chroma, output_path: str = None):&#10;    &quot;&quot;&quot;&#10;    Read one feature file, parse scenarios, for each scenario retrieve context and ask LLM to rewrite descriptively.&#10;    &quot;&quot;&quot;&#10;    scenarios = parse_feature_file(path)&#10;    rewritten_sections = []&#10;    for s in scenarios:&#10;        origin = f&quot;Feature: {s['feature']}\nScenario: {s['scenario']}\n&quot; + &quot;\n&quot;.join(s['steps'])&#10;        # Find matches for the scenario text&#10;        matches = find_matches(vectordb, origin, k=5)&#10;        # collect preserve phrases (unique step lines from matches)&#10;        preserve = set()&#10;        context_snippets = []&#10;        for m in matches:&#10;            context_snippets.append(f&quot;[{m['metadata']['file']}::{m['metadata']['scenario']}] {m['text']}&quot;)&#10;            for st in m['steps']:&#10;                # keep only meaningful short steps; strip leading whitespace&#10;                preserve.add(st.strip())&#10;        preserve_list = &quot;\n&quot;.join(sorted(preserve))&#10;        context = &quot;\n\n&quot;.join(context_snippets) if context_snippets else &quot;&quot;&#10;        prompt = REWRITE_PROMPT.format(preserve_list=preserve_list, context=context, origin=origin)&#10;        rewritten = call_llm(prompt)&#10;        rewritten_sections.append(rewritten.strip())&#10;        print(f&quot;Rewrote scenario: {s['scenario']}&quot;)&#10;    out = &quot;\n\n&quot;.join(rewritten_sections)&#10;    if not output_path:&#10;        output_path = path.replace(&quot;.feature&quot;, &quot;.descriptive.feature&quot;)&#10;    # Ensure output directory exists&#10;    out_dir = os.path.dirname(output_path)&#10;    if out_dir and not os.path.exists(out_dir):&#10;        os.makedirs(out_dir, exist_ok=True)&#10;    with open(output_path, 'w', encoding='utf-8') as f:&#10;        f.write(out)&#10;    print(f&quot;Wrote rewritten file to {output_path}&quot;)&#10;&#10;&#10;def new_scenario_from_requirement(requirement: str, vectordb: Chroma, top_k: int = 5, output_path: str = None):&#10;    &quot;&quot;&quot;&#10;    Given a textual requirement, retrieve matches and ask LLM to generate a new, consistent scenario.&#10;    Also returns the list of matching existing scenarios, with a consistency check.&#10;    &quot;&quot;&quot;&#10;    matches = find_matches(vectordb, requirement, k=top_k)&#10;    preserve = set()&#10;    context_snippets = []&#10;    matches_list = []&#10;    print(&quot;\n--- Consistency Check: Similar Existing Scenarios ---&quot;)&#10;    threshold = 0.8  # similarity threshold for warning&#10;    found_high = False&#10;    for m in matches:&#10;        context_snippets.append(f&quot;[{m['metadata']['file']}::{m['metadata']['scenario']}] {m['text']}&quot;)&#10;        matches_list.append(f&quot;{m['metadata']['file']}::{m['metadata']['scenario']} (score={m['score']:.4f})&quot;)&#10;        for st in m['steps']:&#10;            preserve.add(st.strip())&#10;        print(f&quot;- Feature: {m['metadata'].get('feature', '')}&quot;)&#10;        print(f&quot;  Scenario: {m['metadata'].get('scenario', '')}&quot;)&#10;        print(f&quot;  Score: {m['score']:.4f}&quot;)&#10;        print(f&quot;  Summary: {m['summary']}&quot;)&#10;        print()&#10;        if m['score'] &gt;= threshold:&#10;            found_high = True&#10;    if found_high:&#10;        print(&quot;WARNING: One or more similar scenarios have a high similarity score. This requirement may already be covered.\n&quot;)&#10;    prompt = NEW_SCENARIO_PROMPT.format(&#10;        preserve_list=&quot;\n&quot;.join(sorted(preserve)),&#10;        context=&quot;\n\n&quot;.join(context_snippets),&#10;        requirement=requirement&#10;    )&#10;    generated = call_llm(prompt)&#10;    # Save or print&#10;    if output_path:&#10;        with open(output_path, 'w', encoding='utf-8') as f:&#10;            f.write(generated)&#10;        print(f&quot;Wrote generated scenario to {output_path}&quot;)&#10;    print(&quot;\n--- Generated Scenario ---\n&quot;)&#10;    print(generated)&#10;    print(&quot;\n--- Matching existing scenarios (top results) ---&quot;)&#10;    for m in matches_list:&#10;        print(&quot;-&quot;, m)&#10;&#10;&#10;# -----------------------&#10;# CLI&#10;# -----------------------&#10;def main():&#10;    parser = argparse.ArgumentParser(description=&quot;RAG BDD tool&quot;)&#10;    sub = parser.add_subparsers(dest='cmd')&#10;&#10;    p_idx = sub.add_parser('index', help='Index .feature files into Chroma')&#10;    p_idx.add_argument('--path', required=True, help='.feature file or directory')&#10;    p_idx.add_argument('--persist', default='./chroma_db', help='Chroma persist directory')&#10;&#10;    p_rewrite = sub.add_parser('rewrite', help='Rewrite a feature file descriptively')&#10;    p_rewrite.add_argument('--file', required=True, help='path to feature file to rewrite')&#10;    p_rewrite.add_argument('--persist', default='./chroma_db', help='Chroma persist directory')&#10;    p_rewrite.add_argument('--out', default=None, help='output file')&#10;&#10;    p_new = sub.add_parser('new', help='Create new scenario from requirement')&#10;    p_new.add_argument('--requirement', required=True, help='requirement text')&#10;    p_new.add_argument('--persist', default='./chroma_db', help='Chroma persist directory')&#10;    p_new.add_argument('--top', type=int, default=5, help='top-k matches')&#10;    p_new.add_argument('--out', default=None, help='output file to save scenario')&#10;&#10;    args = parser.parse_args()&#10;    if args.cmd == 'index':&#10;        scenarios = parse_path(args.path)&#10;        build_index(scenarios, persist_dir=args.persist)&#10;    elif args.cmd == 'rewrite':&#10;        # load vectordb&#10;        emb = HuggingFaceEmbeddings(model_name=&quot;all-MiniLM-L6-v2&quot;)&#10;        vectordb = Chroma(persist_directory=args.persist, embedding_function=emb)&#10;        rewrite_feature_file(args.file, vectordb, output_path=args.out)&#10;    elif args.cmd == 'new':&#10;        emb = HuggingFaceEmbeddings(model_name=&quot;all-MiniLM-L6-v2&quot;)&#10;        vectordb = Chroma(persist_directory=args.persist, embedding_function=emb)&#10;        new_scenario_from_requirement(args.requirement, vectordb, top_k=args.top, output_path=args.out)&#10;    else:&#10;        parser.print_help()&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>